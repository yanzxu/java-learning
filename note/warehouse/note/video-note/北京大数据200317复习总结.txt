一、数仓的概念
    数仓的数据哪来的，最终去了哪？
    输入：前端埋点产生日志用户行为数据  javaee后台产生的业务数据
    输出：报表系统（绝大多数公司） 、用户画像、推荐系统

二、需求
    数仓项目（采集+数仓建模+数据分析）
    
    项目架构怎么设计
    数据：
        用户行为数据（文件形式存在，放在日志服务器里面）
            每天100万日活，每天100g数据左右
        javaee数据（mysql里面）
三、项目架构设计（0-1）

    采集通道：文件形式存在
           日志文件=》 flume =>Kafka=>flume/java=>HDFS 
           mysql =》sqoop/datax/kettle=>HDFS 
    数仓建模：
        HDFS => Hive/spark
    数据分析：即席查询（kylin /presto）   

    可视化：superset
    
    权限管理：张三、李四（老板）
    元数据管理：atlas
    数据质量：shell  griffin
    
    
    部署多少台服务器？ 10台
    1）用户行为数据
        100万日活： 每天产生的日志100条，1条日志大小：0.5-2k =>1k
        100万* 100条 * 1k =100g
        
        ods 100g=》 10g
        dwd 10g
        dws +ads => 50g
        ads忽略不计
        70g 
        70g*半年不扩容180天* 3副本/0.7=
    2）kafka里面的数据
        100g * 2副本 * 3天 /0.7= 
    
    3)javaEE后台数据量多少
        100万日活：  10万   10-20条  每条日志1k   
        10万 *10条 *1k =1g
        1g * 3  
        3g * 180天 * 3个副本 /0.7 
        
        
        用户行为数据 +kafka里面的数据 + javaEE后台数据量 =  56T 
        
    4)一台服务器的配置：
        128g内存  20核 40线程   8* 2.4t=19.2t 
        https://item.jd.com/67886744653.html#crumb-wrap
        
        56t/19.2 = 3台
        
        （1）存储空间影响购买服务器个数：
            在企业正常情况，至少1年不扩容：3台 * 2 =6台
        （2）并发度：hadoop/hive/sqoop、  
            40线程 *  6台 = 240线程
            剩余200个线程参与项目运算
        （3）内存： hadoop/hive/sqoop、
             128g * 6台 = 768g  650g
             
             128m数据-》1g内存
             1g =》8g
             81g  -》650g
             
             4.3万* 6 = 25.8万   空调  2000   电费  1.5元
             服务器机柜
             https://item.jd.com/100003142313.html
             房租
 
    
    5）每台服务器上部署哪些框架？10台   5台
        日志服务器配置：
        （springboot接收数据，写入磁盘 flume） 16g 、32g/ 64g
    
        （1）消耗内存的尽量分开
        （2）数据传输比较紧密的放在一起
        （3）客户端尽量放在1-2台服务器
    
    6）购买阿里云还是物理机？  
        购物物理机： https://item.jd.com/67886744653.html#crumb-wrap
        服务器机柜;https://item.jd.com/100003142313.html
        购买品牌机：
        
        
        阿里云：降低配置：40万
        93万   40万

    6）大概需要多钱？    去哪去买？
        物理机 京东  阿里云官方  腾讯云    华为云
    
    
    7）每一个功能模块需要多久完成？
        
        测试集群、生产集群
    
        首批指标查询用时
    8）选择什么框架  CDH  HDP  apache
    
 
四、Kafka
    1、基本信息
        （1）组成
        （2）安装几台kafka
            2 * （生产者峰值生产速率 * 副本  /100 ） + 1 = 3台
        （3）压测：生产者峰值生产速率 、消费者峰值消费速率   
        （4）副本：2-3个副本  2个副本的居多
            副本越多：可靠性越高，增加了网络传输，降低了效率
        （5）kafka中数据保存多久
            默认7天  =》3天
        （6）数据量：
            100万日活  100条日志   =》100万*100条=1亿
            
            速率=1亿条/（24 * 3600）= 1150条/s
            1k * 1150条/s = 1m/s 
            
            峰值传输速率：晚上  8点 手纸   20m/s   不要超过50m/s
        （7） 磁盘多大
            100g  * 2个副本  * 3天 / 0.7 = 
        （8）分区数设置多少个
            先设置一个分区
                压测=》生产者峰值生产速率（tp） 、消费者峰值消费速率(tc) 
              自己期望的传输速率 （吞吐量） t 
            分区数= t /min(tp,tc) = 100/20 = 5个分区
            
            t = 100m/s   tp=20m/s  tc = 40m/s
    
    2、挂了
        短期flume  channenl有缓冲作用
        长期，有日志数据30天
        重启kafka
    
    3、丢失数据
        ack 
        0   生产者发送过来数据就不管了   效率最高  可靠性最差，在企业几乎不用
        1   发过来的数据，leader应答    效率一般  可靠性一般
        -1  发送过来的数据 leader和follower共同应答   效率最低  可靠性最高
        
        1 =》传输普通日志，丢几条  无所谓   本次项目选择这个
        -1=》金融行业，或者对钱比较敏感，要求可靠性比较高， 选择-1
        
    4、重复了
        （1）不处理
        
        （2）处理
            自身  ：  事务+幂等性 + ack =-1  可靠性高，但是效率非常低，  在项目中用的比较少
            找兄弟：在下一级处理  hive的dwd层或者实时中sparkstreaming

    5、积压了
        自身：增加分区数， 消费者要增加CPU核数
                6个分区 6CPU   spark  提交参数  CPU  内存
        
        找兄弟：增加下一级消费者 消费速度
                batchsize  1000条/s  =》  2000-3000条/s 
    
    6、优化
        （1）副本  默认是一个=》2个
        （2）保存天数 7天 =》 3天  OPPO 6小时（实时数仓）
        （3）延迟通信时间
        （4）kafka是可以支持压缩  snappy  lz4  none gzip
        （5）调整内存：默认1g =>4-6g
    
    7、杂七杂八
        （1）Kafka高效读写数据
            kafka是集群 分区 
            顺序读写 600m/s   随机读写 100m/s
            1 1  1 1 1 1      1     3    5  
            采用零拷贝技术
        
        发生一条日志如果2m，kafka会出现什么问题？
        
五、Sqoop
    （1）null问题
        mysql null    => hive \N
    （2）sqoop每天向HDFS导入多少数据
        业务数据1g 
        每天导入订单表多少数据？
        1g / 30张  = 34m  * 3-5倍
    
    （3）每天什么时候执行？ 执行多久
        00:30     40分-50分
    
    （4）参数
        mysql      HDFS
         
        连接mysql  
            服务器地址
            用户名
            密码
        HDFS
            路径
            输出路径处理
        quary " $2  and com..."    
         同步策略：
         全量 （数据量比较小、码表）  where  1 =1 
         、新增（数据量比较大，不变化）  createtime= 今天   事务型事实表
         、新增和变化（数据量比较大、变化） createtime= 今天 or operatetime=今天
         、特殊 （一共就到一次）
    
        
        节日：元旦、春节、情人节、
        3.8  清明节、五一、6.1  7.1 8.1  中秋节、10.1  
        6.18  11.11   周末  1024  12.12
        新产品、新市场开拓
        
        
       
        
        ods_用户行为
        ods_业务_订单/登录注册/物流
        
六、  数仓分层
    1）数仓为什么要分层 
        （1）隔离原始数据
        （2）减少重复开发
        （3）把复杂问题简单化
    2）数据集市与数据仓库概念 （了解）
    
    3）数仓命名规范（开发时重点掌握）
    （多人开发，一定要有一套规范）

七、数仓理论
    1）三范式（了解）
        （1）属性不可切割  5台电脑
        （2）不能存在部分函数依赖  AB=》C   A or B =>C 
        （3）不能存在传递函数依赖  A => B => C  C 不能推出A
    2）OLTP和OLAP区别
        OLTP	mysql  oracle   
        OLAP    hive  spark  flink  mr   海量数据的查询 分析
      
    3）采用哪种方式建模的（重点）
        关系建模：ER模型，  mysql等业务数据库的建模
        维度建模：
        星型模型（事实表周围只有一级维度）
        雪花模型（事实表周围有多级维度）
        星座（多个事实表）
    4）事实表和维度表 （开发的重点）
        维度表特点： 
        都是描述信息、通常是概念名称（时间、地点、人物、商品、活动、优惠劵）
        列比较多。  数据量相对比较小
        
        事实表特点：
        通常是动作（下单、支付、退款、点赞、评论、收藏、加购）
        列比较少。数据量比较大
        字段：各种维度表的id+度量值
            user_id,
            spu_id,
            sku_id,
            huodong_id
            time_id
            area_id,
            个数、次数、件数、金额
            订单金额
            订单次数
        
        （1）事务型事实表 ：数据一旦产生就不会发生变化
            （支付流水表、订单详情）
        
        （2）周期快照事实表：周期性的记录
            每天、周、月  主要处理数据发生变化，数据量还比较大
            购物车里面的商品个数经常变化  00:00  这一时刻是多少
            
        （3）累积型快照事实表
            记录的是某件事从开始到结束，各个时间点信息
            订单
            下单 支付  发货   签收的
            1      2    3     4
    5）数仓建模
        （1）ODS层
            创建支持LZO压缩的表；  为了减少存储空间  100g =>10g左右/5g
            创建分区表： 防止全表扫描(导数据：全量、新增、新增和变化)
            保持数据原貌不做任何修改。  起到数据备份的作用，不要解析数据
        （2）DWD层
            a）选择业务过程：选择感兴趣的业务过程。（统计的需求指标需要用到的业务）
            中小型公司：全部业务过程全拿过来
            中大型公司：平安：1000张表  （用200张表统计 100个指标）
           
           b)声明最小粒度
               订单：100元   尚硅谷
                        1个订单 100元
               订单详情：  
                            手纸 5块  * 2 个   =10
                            面膜  10块  * 5 =50 
                            海狗人参丸 40 * 1 个 40
                            
                            id + 度量值
                            
                统计今天手纸买了多少钱 用订单详情  最小粒度  能统计的指标更多
                
                统计今天辽宁地区，所有商品一共卖了多少钱   
                可以用订单表计算最快   用订单详情也能搞定。
            怎么实现：在DWD层不做聚合操作。
        
            c）确定维度
            d）确定事实  度量值（个数、次数、件数、金额）
        
        （3）DWS层
            每天聚合统计
            建什么表： 维度宽表  
            每个表什么字段：站在维度的角度去看事实表，看事实表的度量值
        
        （4）DWT层
            从事情第一次产生，一直到当前这一时刻，累积发生的度量值
            
            建什么表：维度主题宽表 
            每个表什么字段：
            站在维度的角度去看事实表，
            关注事情开始、结束、累积的度量值、一段时间的累积度量值
        
        
        什么时候创建外部表？
            在数仓当中绝大多数表都是外部表；
        只有自己临时使用的表是内部表；
        
        删除数据：元数据、原始数据
        内部表删数据：元数据、原始数据
        外部表删数据：元数据
        
        spark3.0.0 =>hive2.3.7  =》hadoop2.0
        
        hive3.1.2 =>spark3.0 （用存净版） =>hadoop3.0以上
                
        hive3.1.2  hadoop2.0
        
        1.要使用纯净版spark 为什么还要上传非纯净版spark 它们两个的使用位置不同吗？    
        纯净版spark，用来放到yarn上计算。
        非纯净版spark，用来hive通信，创建spark session
        
        2.容量调度配置的那个0.1是指每个application可以使用的资源占集群的最大比例吗？ 如果一个队列中有多个application 结果第一个application就需要占用资源是集群的0.1 那这个队列中后面的app都会卡死对吗？
       
     6）埋点
        在企业里面页面非常多：（中大型公司）动作、曝光、错误、页面、公共信息
        中小型公司（页面比较少）：一个页面一张表
        （商品列表、商品点击、广告、故障、后台活跃、通知、点赞、评论、收藏）
        
    7）列式存储的好处  ：查询速度快；列式存储+压缩=》压缩的会更小
        id  name  age
        1   zs    18
        2   li     19
    行：   1   zs    18   2   li     19
    列式： 1   2      zs   li   18  19
    
    select name from user 
        
        
    8）自定义UDTF好处？
        更加灵活调试，快速定位问题。
        
    9）自定义函数声明问题？
        脚本中执行 自定义函数前边也得加库名 第一遍自己写没加执行不过去
        
    
    
    
        
        
        
        
        
        
        
        